import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from core.optimization_algorithms_engine import OptimizationAlgorithmsEngine, OptimizationType

def render_optimization_panel():
    """Render the optimization algorithms interface"""
    st.header("Advanced Optimization Algorithms")
    
    tabs = st.tabs([
        "Gradient-Based",
        "Evolutionary",
        "Multi-Objective",
        "Constrained",
        "Robust/Stochastic"
    ])
    
    with tabs[0]:
        render_gradient_based()
    
    with tabs[1]:
        render_evolutionary()
        
    with tabs[2]:
        render_multi_objective()
        
    with tabs[3]:
        render_constrained()
        
    with tabs[4]:
        render_robust_stochastic()

def render_gradient_based():
    """Gradient-based optimization methods"""
    st.subheader("Gradient-Based Optimization")
    
    method = st.selectbox(
        "Select method",
        ["L-BFGS-B", "Conjugate Gradient", "Newton's Method"]
    )
    
    # Test function selection
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Test Function**")
        func_type = st.selectbox(
            "Function",
            ["Rosenbrock", "Sphere", "Rastrigin", "Ackley", "Custom"]
        )
        
        dim = st.slider("Dimensions", 2, 20, 2)
        
        # Initial point
        st.write("**Initial Point**")
        init_type = st.selectbox("Initialization", ["Random", "Fixed", "Custom"])
        
        if init_type == "Fixed":
            x0 = np.ones(dim) * 2.0
        elif init_type == "Random":
            x0 = np.random.randn(dim) * 2
        else:
            x0 = []
            for i in range(min(dim, 5)):  # Show only first 5 for large dims
                x0.append(st.number_input(f"x₀[{i}]", value=1.0, key=f"grad_x0_{i}"))
            if dim > 5:
                x0.extend([1.0] * (dim - 5))
            x0 = np.array(x0)
    
    with col2:
        st.write("**Algorithm Settings**")
        
        if method == "L-BFGS-B":
            m = st.slider("Memory size (m)", 3, 20, 10)
            use_bounds = st.checkbox("Use bounds")
            if use_bounds:
                lower_bound = st.number_input("Lower bound", value=-5.0)
                upper_bound = st.number_input("Upper bound", value=5.0)
                bounds = [(lower_bound, upper_bound)] * dim
            else:
                bounds = None
        
        max_iter = st.number_input("Max iterations", 100, 5000, 1000)
        tol = st.number_input("Tolerance", value=1e-8, format="%.2e")
    
    # Define test functions
    if func_type == "Rosenbrock":
        def f(x):
            return sum(100*(x[i+1] - x[i]**2)**2 + (1-x[i])**2 for i in range(len(x)-1))
        def grad_f(x):
            g = np.zeros_like(x)
            for i in range(len(x)-1):
                g[i] += -400*x[i]*(x[i+1] - x[i]**2) - 2*(1-x[i])
                g[i+1] += 200*(x[i+1] - x[i]**2)
            return g
        optimal_val = 0.0
        
    elif func_type == "Sphere":
        def f(x):
            return np.sum(x**2)
        def grad_f(x):
            return 2*x
        optimal_val = 0.0
        
    elif func_type == "Rastrigin":
        def f(x):
            return 10*len(x) + np.sum(x**2 - 10*np.cos(2*np.pi*x))
        def grad_f(x):
            return 2*x + 20*np.pi*np.sin(2*np.pi*x)
        optimal_val = 0.0
        
    elif func_type == "Ackley":
        def f(x):
            a, b, c = 20, 0.2, 2*np.pi
            n = len(x)
            return -a*np.exp(-b*np.sqrt(np.sum(x**2)/n)) - np.exp(np.sum(np.cos(c*x))/n) + a + np.e
        def grad_f(x):
            a, b, c = 20, 0.2, 2*np.pi
            n = len(x)
            term1 = np.sqrt(np.sum(x**2)/n)
            g1 = a*b*np.exp(-b*term1) * x / (n*term1)
            g2 = c*np.sin(c*x) * np.exp(np.sum(np.cos(c*x))/n) / n
            return g1 + g2
        optimal_val = 0.0
    else:
        st.warning("Please select a predefined function")
        return
    
    if st.button("Optimize"):
        try:
            engine = OptimizationAlgorithmsEngine()
            engine.max_iterations = max_iter
            engine.tolerance = tol
            
            with st.spinner("Optimizing..."):
                if method == "L-BFGS-B":
                    result = engine.lbfgs_b(f, grad_f, x0, bounds, m)
                else:
                    st.error("Method not yet implemented")
                    return
            
            # Display results
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Results**")
                st.write(f"Optimal value: {result.f_opt:.6f}")
                st.write(f"True optimal: {optimal_val:.6f}")
                st.write(f"Error: {abs(result.f_opt - optimal_val):.2e}")
                st.write(f"Iterations: {result.iterations}")
                st.write(f"Converged: {result.converged}")
                
                if dim <= 10:
                    st.write("**Optimal point:**")
                    for i in range(min(dim, 5)):
                        st.write(f"x[{i}] = {result.x_opt[i]:.6f}")
                    if dim > 5:
                        st.write("...")
            
            with col2:
                # Convergence plot
                if result.history:
                    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))
                    
                    # Function values
                    ax1.semilogy(result.history['f_vals'])
                    ax1.set_xlabel('Iteration')
                    ax1.set_ylabel('f(x)')
                    ax1.set_title('Function Value Convergence')
                    ax1.grid(True, alpha=0.3)
                    
                    # Gradient norms
                    ax2.semilogy(result.history['grad_norms'])
                    ax2.set_xlabel('Iteration')
                    ax2.set_ylabel('||∇f||')
                    ax2.set_title('Gradient Norm Convergence')
                    ax2.grid(True, alpha=0.3)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
            
            # 2D visualization for 2D problems
            if dim == 2 and st.checkbox("Show 2D landscape"):
                x_range = result.x_opt[0] + np.linspace(-2, 2, 100)
                y_range = result.x_opt[1] + np.linspace(-2, 2, 100)
                X, Y = np.meshgrid(x_range, y_range)
                Z = np.zeros_like(X)
                
                for i in range(X.shape[0]):
                    for j in range(X.shape[1]):
                        Z[i, j] = f(np.array([X[i, j], Y[i, j]]))
                
                fig = plt.figure(figsize=(10, 8))
                ax = fig.add_subplot(111, projection='3d')
                ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)
                
                # Plot optimization path
                if result.history and len(result.history['f_vals']) > 1:
                    # Would need to store x history for this
                    pass
                
                ax.scatter([result.x_opt[0]], [result.x_opt[1]], [result.f_opt], 
                          color='red', s=100, label='Optimum')
                ax.set_xlabel('x₁')
                ax.set_ylabel('x₂')
                ax.set_zlabel('f(x)')
                ax.set_title(f'{func_type} Function Landscape')
                st.pyplot(fig)
                
        except Exception as e:
            st.error(f"Optimization error: {str(e)}")

def render_evolutionary():
    """Evolutionary optimization algorithms"""
    st.subheader("Evolutionary Algorithms")
    
    method = st.selectbox(
        "Select algorithm",
        ["Genetic Algorithm", "Particle Swarm", "Differential Evolution", "Simulated Annealing"]
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Problem Setup**")
        
        # Function selection
        func_type = st.selectbox(
            "Test function",
            ["Schwefel", "Griewank", "Levy", "Michalewicz", "Custom"],
            key="evo_func"
        )
        
        dim = st.slider("Dimensions", 2, 30, 5, key="evo_dim")
        
        # Bounds
        st.write("**Variable Bounds**")
        bound_type = st.selectbox("Bound type", ["Symmetric", "Custom"])
        if bound_type == "Symmetric":
            bound = st.number_input("Bound magnitude", value=10.0)
            bounds = [(-bound, bound)] * dim
        else:
            bounds = [(-10, 10)] * dim  # Simplified
    
    with col2:
        st.write("**Algorithm Parameters**")
        
        if method == "Genetic Algorithm":
            pop_size = st.slider("Population size", 20, 200, 50)
            crossover_prob = st.slider("Crossover probability", 0.5, 1.0, 0.8)
            mutation_prob = st.slider("Mutation probability", 0.01, 0.3, 0.1)
            
        elif method == "Particle Swarm":
            swarm_size = st.slider("Swarm size", 20, 200, 50)
            w = st.slider("Inertia weight", 0.4, 0.9, 0.7)
            c1 = st.slider("Cognitive parameter", 1.0, 2.5, 1.5)
            c2 = st.slider("Social parameter", 1.0, 2.5, 1.5)
            
        elif method == "Simulated Annealing":
            T0 = st.number_input("Initial temperature", value=100.0)
            cooling_rate = st.slider("Cooling rate", 0.8, 0.99, 0.95)
        
        max_iter = st.number_input("Max iterations", 100, 5000, 1000, key="evo_iter")
    
    # Define test functions
    if func_type == "Schwefel":
        def f(x):
            return 418.9829 * len(x) - np.sum(x * np.sin(np.sqrt(np.abs(x))))
        optimal_val = 0.0
        
    elif func_type == "Griewank":
        def f(x):
            sum_sq = np.sum(x**2) / 4000
            prod_cos = np.prod(np.cos(x / np.sqrt(np.arange(1, len(x)+1))))
            return sum_sq - prod_cos + 1
        optimal_val = 0.0
        
    elif func_type == "Levy":
        def f(x):
            w = 1 + (x - 1) / 4
            term1 = np.sin(np.pi * w[0])**2
            term3 = (w[-1] - 1)**2 * (1 + np.sin(2 * np.pi * w[-1])**2)
            wi = w[:-1]
            sum_term = np.sum((wi - 1)**2 * (1 + 10 * np.sin(np.pi * wi + 1)**2))
            return term1 + sum_term + term3
        optimal_val = 0.0
        
    elif func_type == "Michalewicz":
        def f(x):
            m = 10
            return -np.sum(np.sin(x) * np.sin((np.arange(1, len(x)+1) * x**2) / np.pi)**(2*m))
        optimal_val = -1.8013 if dim == 2 else None  # Depends on dimension
    else:
        st.warning("Please select a predefined function")
        return
    
    if st.button("Run Optimization", key="evo_run"):
        try:
            engine = OptimizationAlgorithmsEngine()
            engine.max_iterations = max_iter
            
            with st.spinner(f"Running {method}..."):
                if method == "Genetic Algorithm":
                    result = engine.genetic_algorithm(f, bounds, pop_size, 
                                                    crossover_prob, mutation_prob)
                elif method == "Particle Swarm":
                    result = engine.particle_swarm_optimization(f, bounds, swarm_size,
                                                              w, c1, c2)
                elif method == "Simulated Annealing":
                    result = engine.simulated_annealing(f, bounds, None, T0, cooling_rate)
                else:
                    st.error("Method not implemented")
                    return
            
            # Results
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Results**")
                st.write(f"Best value found: {result.f_opt:.6f}")
                if optimal_val is not None:
                    st.write(f"Known optimal: {optimal_val:.6f}")
                    st.write(f"Error: {abs(result.f_opt - optimal_val):.2e}")
                st.write(f"Iterations: {result.iterations}")
                
                if dim <= 10:
                    st.write("**Best solution:**")
                    for i in range(min(dim, 5)):
                        st.write(f"x[{i}] = {result.x_opt[i]:.6f}")
                    if dim > 5:
                        st.write("...")
            
            with col2:
                # Convergence plot
                if result.history:
                    fig, ax = plt.subplots(figsize=(8, 6))
                    
                    if 'best_fitness' in result.history:
                        ax.plot(result.history['best_fitness'], label='Best')
                    if 'avg_fitness' in result.history:
                        ax.plot(result.history['avg_fitness'], label='Average', alpha=0.5)
                    if 'global_best' in result.history:
                        ax.plot(result.history['global_best'], label='Global Best')
                    if 'temperature' in result.history and method == "Simulated Annealing":
                        ax2 = ax.twinx()
                        ax2.plot(result.history['temperature'], 'r--', alpha=0.5, label='Temperature')
                        ax2.set_ylabel('Temperature')
                        ax2.legend(loc='upper right')
                    
                    ax.set_xlabel('Iteration')
                    ax.set_ylabel('Objective Value')
                    ax.set_title(f'{method} Convergence')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    st.pyplot(fig)
                    
        except Exception as e:
            st.error(f"Optimization error: {str(e)}")

def render_multi_objective():
    """Multi-objective optimization"""
    st.subheader("Multi-Objective Optimization")
    
    st.write("**NSGA-II Algorithm**")
    
    # Problem selection
    problem = st.selectbox(
        "Test problem",
        ["ZDT1", "ZDT2", "ZDT3", "DTLZ1", "Custom"]
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        if problem == "ZDT1":
            st.write("**ZDT1 Problem**")
            st.latex(r"f_1(x) = x_1")
            st.latex(r"f_2(x) = g(x)[1 - \sqrt{x_1/g(x)}]")
            st.latex(r"g(x) = 1 + 9\sum_{i=2}^n x_i/(n-1)")
            
            n_vars = st.slider("Number of variables", 2, 30, 10)
            bounds = [(0, 1)] * n_vars
            
            def f1(x):
                return x[0]
            def f2(x):
                g = 1 + 9 * np.sum(x[1:]) / (len(x) - 1)
                return g * (1 - np.sqrt(x[0] / g))
            
            objectives = [f1, f2]
            
        elif problem == "ZDT2":
            st.write("**ZDT2 Problem**")
            st.latex(r"f_1(x) = x_1")
            st.latex(r"f_2(x) = g(x)[1 - (x_1/g(x))^2]")
            
            n_vars = st.slider("Number of variables", 2, 30, 10)
            bounds = [(0, 1)] * n_vars
            
            def f1(x):
                return x[0]
            def f2(x):
                g = 1 + 9 * np.sum(x[1:]) / (len(x) - 1)
                return g * (1 - (x[0] / g)**2)
            
            objectives = [f1, f2]
        else:
            st.info("Other problems not yet implemented")
            return
    
    with col2:
        st.write("**Algorithm Settings**")
        pop_size = st.slider("Population size", 50, 300, 100)
        n_generations = st.slider("Generations", 50, 500, 200)
    
    if st.button("Run NSGA-II"):
        try:
            engine = OptimizationAlgorithmsEngine()
            engine.population_size = pop_size
            engine.max_iterations = n_generations
            
            with st.spinner("Running NSGA-II..."):
                result = engine.nsga_ii(objectives, bounds, pop_size)
            
            # Plot Pareto front
            if result.pareto_front is not None:
                # Evaluate objectives for Pareto front
                pareto_obj = np.array([
                    [obj(x) for obj in objectives] 
                    for x in result.pareto_front
                ])
                
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.scatter(pareto_obj[:, 0], pareto_obj[:, 1], alpha=0.6)
                ax.set_xlabel('f₁(x)')
                ax.set_ylabel('f₂(x)')
                ax.set_title(f'Pareto Front - {problem}')
                ax.grid(True, alpha=0.3)
                
                # True Pareto front for ZDT problems
                if problem in ["ZDT1", "ZDT2"]:
                    x1_true = np.linspace(0, 1, 100)
                    if problem == "ZDT1":
                        f2_true = 1 - np.sqrt(x1_true)
                    else:
                        f2_true = 1 - x1_true**2
                    ax.plot(x1_true, f2_true, 'r--', label='True Pareto front')
                    ax.legend()
                
                st.pyplot(fig)
                
                st.write(f"**Results**")
                st.write(f"Pareto front size: {len(result.pareto_front)}")
                st.write(f"Iterations: {result.iterations}")
                
                # Download Pareto front
                if st.checkbox("Show Pareto solutions"):
                    st.dataframe({
                        'Solution': [f"x{i}" for i in range(len(result.pareto_front))],
                        'f₁': pareto_obj[:, 0],
                        'f₂': pareto_obj[:, 1]
                    })
                    
        except Exception as e:
            st.error(f"Multi-objective optimization error: {str(e)}")

def render_constrained():
    """Constrained optimization"""
    st.subheader("Constrained Optimization")
    
    st.write("**Sequential Quadratic Programming (SQP)**")
    
    # Problem type
    problem = st.selectbox(
        "Test problem",
        ["Simple constrained", "Engineering design", "Custom"]
    )
    
    if problem == "Simple constrained":
        st.write("**Problem:**")
        st.latex(r"\min_{x,y} \quad (x-2)^2 + (y-1)^2")
        st.latex(r"\text{s.t.} \quad x^2 + y^2 \leq 1")
        st.latex(r"\quad\quad\quad x + y = 1")
        
        def f(x):
            return (x[0] - 2)**2 + (x[1] - 1)**2
        
        def grad_f(x):
            return np.array([2*(x[0] - 2), 2*(x[1] - 1)])
        
        constraints = [
            {
                'type': 'ineq',
                'fun': lambda x: 1 - x[0]**2 - x[1]**2,
                'jac': lambda x: np.array([-2*x[0], -2*x[1]])
            },
            {
                'type': 'eq',
                'fun': lambda x: x[0] + x[1] - 1,
                'jac': lambda x: np.array([1, 1])
            }
        ]
        
        col1, col2 = st.columns(2)
        with col1:
            x0_1 = st.number_input("Initial x", value=0.5)
            x0_2 = st.number_input("Initial y", value=0.5)
            x0 = np.array([x0_1, x0_2])
        
        with col2:
            max_iter = st.number_input("Max iterations", 50, 500, 100)
        
        if st.button("Solve"):
            try:
                engine = OptimizationAlgorithmsEngine()
                engine.max_iterations = max_iter
                
                with st.spinner("Solving constrained problem..."):
                    result = engine.sequential_quadratic_programming(
                        f, grad_f, constraints, x0
                    )
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Results**")
                    st.write(f"Optimal point: ({result.x_opt[0]:.4f}, {result.x_opt[1]:.4f})")
                    st.write(f"Optimal value: {result.f_opt:.4f}")
                    st.write(f"Constraint violation: {result.constraint_violation:.2e}")
                    st.write(f"Iterations: {result.iterations}")
                    st.write(f"Converged: {result.converged}")
                
                with col2:
                    # Visualization
                    x = np.linspace(-1.5, 2.5, 100)
                    y = np.linspace(-1.5, 2.5, 100)
                    X, Y = np.meshgrid(x, y)
                    Z = (X - 2)**2 + (Y - 1)**2
                    
                    fig, ax = plt.subplots(figsize=(8, 8))
                    
                    # Objective contours
                    contour = ax.contour(X, Y, Z, levels=20, alpha=0.6)
                    ax.clabel(contour, inline=True, fontsize=8)
                    
                    # Constraints
                    # Inequality constraint
                    circle = plt.Circle((0, 0), 1, fill=False, color='red', linewidth=2)
                    ax.add_patch(circle)
                    
                    # Equality constraint
                    x_eq = np.linspace(-1, 2, 100)
                    y_eq = 1 - x_eq
                    ax.plot(x_eq, y_eq, 'g-', linewidth=2, label='x + y = 1')
                    
                    # Solution
                    ax.plot(result.x_opt[0], result.x_opt[1], 'ro', markersize=10, 
                           label=f'Solution: ({result.x_opt[0]:.3f}, {result.x_opt[1]:.3f})')
                    
                    # Initial point
                    ax.plot(x0[0], x0[1], 'bo', markersize=8, label='Initial point')
                    
                    ax.set_xlim(-1.5, 2.5)
                    ax.set_ylim(-1.5, 2.5)
                    ax.set_xlabel('x')
                    ax.set_ylabel('y')
                    ax.set_title('Constrained Optimization')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    ax.set_aspect('equal')
                    
                    st.pyplot(fig)
                    
            except Exception as e:
                st.error(f"Constrained optimization error: {str(e)}")
    else:
        st.info("Other constrained problems coming soon!")

def render_robust_stochastic():
    """Robust and stochastic optimization"""
    st.subheader("Robust Optimization")
    
    st.write("**Optimization under uncertainty**")
    
    method = st.selectbox(
        "Method",
        ["Worst-case robust", "Expected value", "Chance-constrained"]
    )
    
    # Simple robust optimization example
    st.write("**Example: Portfolio optimization with uncertain returns**")
    st.latex(r"\min_x \quad f(x, \xi) = -\sum_i \xi_i x_i + \lambda \|x\|^2")
    st.latex(r"\text{where } \xi \text{ are uncertain returns}")
    
    col1, col2 = st.columns(2)
    
    with col1:
        n_assets = st.slider("Number of assets", 2, 10, 4)
        
        # Uncertainty set
        uncertainty_type = st.selectbox(
            "Uncertainty type",
            ["Box uncertainty", "Ellipsoidal", "Normal distribution"]
        )
        
        if uncertainty_type == "Box uncertainty":
            uncertainty_level = st.slider("Uncertainty level (%)", 10, 50, 20)
            nominal_returns = np.random.randn(n_assets) * 0.1 + 0.05
            
            uncertainty_set = {
                'type': 'box',
                'lower': nominal_returns * (1 - uncertainty_level/100),
                'upper': nominal_returns * (1 + uncertainty_level/100)
            }
        elif uncertainty_type == "Normal distribution":
            std_dev = st.slider("Standard deviation", 0.01, 0.1, 0.03)
            nominal_returns = np.random.randn(n_assets) * 0.1 + 0.05
            
            uncertainty_set = {
                'type': 'normal',
                'mean': nominal_returns,
                'std': std_dev
            }
        
        lambda_reg = st.slider("Regularization λ", 0.0, 1.0, 0.1)
    
    with col2:
        st.write("**Nominal returns:**")
        for i in range(min(n_assets, 5)):
            st.write(f"Asset {i+1}: {nominal_returns[i]:.3f}")
        if n_assets > 5:
            st.write("...")
    
    # Bounds for portfolio weights
    bounds = [(0, 1)] * n_assets
    
    # Objective function
    def f(x, xi):
        return -np.dot(xi, x) + lambda_reg * np.dot(x, x)
    
    if st.button("Solve Robust Problem"):
        try:
            engine = OptimizationAlgorithmsEngine()
            
            with st.spinner("Solving robust optimization..."):
                result = engine.robust_optimization(
                    f, bounds, uncertainty_set, 
                    method='worst_case' if method == "Worst-case robust" else 'expected_value'
                )
            
            st.write("**Results**")
            st.write("Optimal portfolio weights:")
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # Portfolio weights
            ax1.bar(range(n_assets), result.x_opt)
            ax1.set_xlabel('Asset')
            ax1.set_ylabel('Weight')
            ax1.set_title('Robust Portfolio Allocation')
            ax1.set_xticks(range(n_assets))
            ax1.set_xticklabels([f'A{i+1}' for i in range(n_assets)])
            
            # Uncertainty bounds on objective
            if result.uncertainty_bounds:
                ax2.hist(np.random.normal(
                    result.f_opt, 
                    (result.uncertainty_bounds[1] - result.uncertainty_bounds[0])/4,
                    1000
                ), bins=50, alpha=0.7, density=True)
                ax2.axvline(result.uncertainty_bounds[0], color='r', linestyle='--', 
                           label=f'5% VaR: {result.uncertainty_bounds[0]:.3f}')
                ax2.axvline(result.uncertainty_bounds[1], color='r', linestyle='--',
                           label=f'95% VaR: {result.uncertainty_bounds[1]:.3f}')
                ax2.axvline(result.f_opt, color='g', linewidth=2,
                           label=f'Expected: {result.f_opt:.3f}')
                ax2.set_xlabel('Portfolio Return')
                ax2.set_ylabel('Probability Density')
                ax2.set_title('Return Distribution')
                ax2.legend()
            
            plt.tight_layout()
            st.pyplot(fig)
            
            # Summary stats
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"Expected return: {-result.f_opt:.3f}")
                st.write(f"Sum of weights: {np.sum(result.x_opt):.3f}")
            with col2:
                if result.uncertainty_bounds:
                    st.write(f"5% VaR: {result.uncertainty_bounds[0]:.3f}")
                    st.write(f"95% VaR: {result.uncertainty_bounds[1]:.3f}")
                    
        except Exception as e:
            st.error(f"Robust optimization error: {str(e)}")